{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bbc450",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:18.169677Z",
     "iopub.status.busy": "2024-09-16T09:54:18.169322Z",
     "iopub.status.idle": "2024-09-16T09:54:18.935173Z",
     "shell.execute_reply": "2024-09-16T09:54:18.934169Z"
    },
    "papermill": {
     "duration": 0.778588,
     "end_time": "2024-09-16T09:54:18.937826",
     "exception": false,
     "start_time": "2024-09-16T09:54:18.159238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/train-data/train_folds.csv\n",
      "/kaggle/input/emo-map-challenge/sample_submission.csv\n",
      "/kaggle/input/emo-map-challenge/train_dataset.csv\n",
      "/kaggle/input/emo-map-challenge/test_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1cd0e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:18.955864Z",
     "iopub.status.busy": "2024-09-16T09:54:18.955383Z",
     "iopub.status.idle": "2024-09-16T09:54:34.638631Z",
     "shell.execute_reply": "2024-09-16T09:54:34.637432Z"
    },
    "papermill": {
     "duration": 15.694925,
     "end_time": "2024-09-16T09:54:34.641238",
     "exception": false,
     "start_time": "2024-09-16T09:54:18.946313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wtfml\r\n",
      "  Downloading wtfml-0.0.3-py3-none-any.whl.metadata (808 bytes)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.10/site-packages (from wtfml) (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->wtfml) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->wtfml) (1.14.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->wtfml) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->wtfml) (3.5.0)\r\n",
      "Downloading wtfml-0.0.3-py3-none-any.whl (10 kB)\r\n",
      "Installing collected packages: wtfml\r\n",
      "Successfully installed wtfml-0.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wtfml #Library used early stoping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae9bd6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:34.659532Z",
     "iopub.status.busy": "2024-09-16T09:54:34.659116Z",
     "iopub.status.idle": "2024-09-16T09:54:41.202340Z",
     "shell.execute_reply": "2024-09-16T09:54:41.201459Z"
    },
    "papermill": {
     "duration": 6.55517,
     "end_time": "2024-09-16T09:54:41.204919",
     "exception": false,
     "start_time": "2024-09-16T09:54:34.649749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.15 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from torch.nn import functional as F\n",
    "from wtfml.utils import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62f5a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:41.224297Z",
     "iopub.status.busy": "2024-09-16T09:54:41.223747Z",
     "iopub.status.idle": "2024-09-16T09:54:44.054739Z",
     "shell.execute_reply": "2024-09-16T09:54:44.053632Z"
    },
    "papermill": {
     "duration": 2.843374,
     "end_time": "2024-09-16T09:54:44.057020",
     "exception": false,
     "start_time": "2024-09-16T09:54:41.213646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AS the data is imbalanced and relativley small, dataset divided into 5 folds,maintaining \n",
    "# the class distribution same as training data set\n",
    "df = pd.read_csv(\"/kaggle/input/emo-map-challenge/train_dataset.csv\")\n",
    "df[\"kfold\"] = -1    \n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "y = df.emotion.values\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f\n",
    "\n",
    "df.to_csv(\"train_folds.csv\", index=False)\n",
    "# this file directly uploaded in the input to use directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaea1b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:44.075903Z",
     "iopub.status.busy": "2024-09-16T09:54:44.075525Z",
     "iopub.status.idle": "2024-09-16T09:54:45.858716Z",
     "shell.execute_reply": "2024-09-16T09:54:45.857844Z"
    },
    "papermill": {
     "duration": 1.795207,
     "end_time": "2024-09-16T09:54:45.861330",
     "exception": false,
     "start_time": "2024-09-16T09:54:44.066123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upon experimenting on all the pretrained versions, I found that RESNET Familiy is doing the best,\n",
    "# so i have started with pretrained versions of those.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetForMultiClass(nn.Module):\n",
    "    def __init__(self, num_classes=7, model_type='resnet152', pretrained=True):\n",
    "        super(ResNetForMultiClass, self).__init__()\n",
    "        \n",
    "        # Select the base model based on the model_type parameter\n",
    "        if model_type == 'resnet101':\n",
    "            self.base_model = models.resnet101(pretrained=pretrained)\n",
    "        elif model_type == 'resnet152':\n",
    "            self.base_model = models.resnet152(pretrained=pretrained)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "        \n",
    "        # Modify the fully connected layer to match the number of classes\n",
    "        self.base_model.fc = nn.Linear(\n",
    "            in_features=self.base_model.fc.in_features,\n",
    "            out_features=num_classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, image, targets=None):\n",
    "        # Forward pass through the base model\n",
    "        out = self.base_model(image)\n",
    "        \n",
    "        # If targets are provided, calculate the loss\n",
    "        if targets is not None:\n",
    "            loss = nn.CrossEntropyLoss()(out, targets)\n",
    "            return out, loss\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f15f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:45.879855Z",
     "iopub.status.busy": "2024-09-16T09:54:45.879340Z",
     "iopub.status.idle": "2024-09-16T09:54:45.890164Z",
     "shell.execute_reply": "2024-09-16T09:54:45.889220Z"
    },
    "papermill": {
     "duration": 0.022206,
     "end_time": "2024-09-16T09:54:45.892293",
     "exception": false,
     "start_time": "2024-09-16T09:54:45.870087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Building Custom Dataset Loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, pixel_arrays, targets, resize=None, augmentations=None):\n",
    "        self.pixel_arrays = pixel_arrays\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixel_arrays)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the 1D array to a 2D grayscale image\n",
    "        image = np.array(self.pixel_arrays[idx], dtype=np.float32).reshape(48, 48)\n",
    "        \n",
    "        # Resize image if needed\n",
    "        if self.resize:\n",
    "            image = cv2.resize(image, self.resize)\n",
    "\n",
    "        # Expand dimensions to (48, 48, 1) and then convert to (new_size, new_size, 3) for RGB\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "        image = np.repeat(image, 3, axis=-1)  # Convert to RGB\n",
    "\n",
    "        # Apply augmentations if specified\n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        # Convert image to PyTorch tensor and permute to (3, new_size, new_size) format\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # For RGB, shape becomes (3, new_size, new_size)\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a7a94b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:45.910255Z",
     "iopub.status.busy": "2024-09-16T09:54:45.909948Z",
     "iopub.status.idle": "2024-09-16T09:54:45.929906Z",
     "shell.execute_reply": "2024-09-16T09:54:45.929098Z"
    },
    "papermill": {
     "duration": 0.031408,
     "end_time": "2024-09-16T09:54:45.931920",
     "exception": false,
     "start_time": "2024-09-16T09:54:45.900512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Methods are responsible for training, validation, and predicting the emotions for the data \n",
    "class Engine:\n",
    "    @staticmethod\n",
    "    def train(data_loader, model, optimizer, device,scheduler=None, accumulation_steps=1, fp16=False):\n",
    "        model.train()\n",
    "        losses = AverageMeter()\n",
    "        scaler = torch.cuda.amp.GradScaler() if fp16 else None\n",
    "        \n",
    "        if accumulation_steps > 1:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for batch_idx, (images, targets) in enumerate(tk0):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                if fp16:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = torch.nn.CrossEntropyLoss()(outputs,targets)\n",
    "                    loss.backward()\n",
    "\n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    scaler.step(optimizer) if fp16 else optimizer.step()\n",
    "                    if scheduler:\n",
    "                        scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            losses.update(loss.item(), data_loader.batch_size)\n",
    "            tk0.set_postfix(loss=losses.avg)\n",
    "        \n",
    "        return losses.avg\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(data_loader, model, device, use_tpu=False):\n",
    "        losses = AverageMeter()\n",
    "        final_predictions = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                images, targets = data  # Adjust if your data format is different\n",
    "\n",
    "                # Move tensors to device\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                predictions, loss = model(images, targets)\n",
    "                predictions = predictions.cpu()\n",
    "                losses.update(loss.item(), images.size(0))\n",
    "                final_predictions.append(predictions)\n",
    "                tk0.set_postfix(loss=losses.avg)\n",
    "\n",
    "        # Concatenate all predictions and convert to NumPy array\n",
    "        final_predictions = torch.cat(final_predictions).numpy()\n",
    "        return final_predictions, losses.avg\n",
    "    def predict(data_loader, model, device, use_tpu=False):\n",
    "        model.eval()\n",
    "        final_predictions = []\n",
    "        with torch.no_grad():\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                inputs, _ = data  # Unpack data\n",
    "                inputs = inputs.to(device)\n",
    "                predictions = model(inputs)  # Assume model returns only predictions\n",
    "                final_predictions.append(predictions.cpu())\n",
    "                tk0.set_postfix()\n",
    "        return torch.cat(final_predictions).numpy()\n",
    "        # Concatenate all predictions and convert to numpy array\n",
    "        return torch.cat(final_predictions).numpy()\n",
    "    \n",
    "#The AverageMeter class is a utility for tracking and calculating the \n",
    "#running average of a metric (such as loss) over multiple updates. \n",
    "#It maintains cumulative statistics, including the current value, sum of values, \n",
    "#count of values, and the average value.prefered to handle multiple batchoutputs in training and validation\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427db0ef",
   "metadata": {
    "papermill": {
     "duration": 0.00799,
     "end_time": "2024-09-16T09:54:45.948147",
     "exception": false,
     "start_time": "2024-09-16T09:54:45.940157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "FP16 (16-bit Floating Point Precision)(However it was set to false)\n",
    "\n",
    "\t•\tDescription: FP16, or half-precision floating point, uses 16-bit numbers instead of the standard 32-bit (FP32). This reduces memory usage and can speed up computations on compatible hardware, improving training efficiency without significantly sacrificing precision.\n",
    "\t•\tUsage: In mixed-precision training, torch.cuda.amp.GradScaler and torch.cuda.amp.autocast are used to manage gradient scaling and automatic casting of operations between FP16 and FP32 for better performance and numerical stability.\n",
    "\n",
    "Scheduler (ReduceLROnPlateau)\n",
    "\n",
    "\t•\tDescription: The ReduceLROnPlateau scheduler adjusts the learning rate based on the performance of the model. It reduces the learning rate when a specified metric (e.g., validation loss) stops improving, helping to fine-tune the model and prevent overfitting.\n",
    "\t•\tUsage: scheduler.step(metrics) is called with the monitored metric to check if the learning rate needs to be adjusted, typically in response to a lack of improvement in the specified metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea54442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:45.966335Z",
     "iopub.status.busy": "2024-09-16T09:54:45.966038Z",
     "iopub.status.idle": "2024-09-16T09:54:45.986630Z",
     "shell.execute_reply": "2024-09-16T09:54:45.985654Z"
    },
    "papermill": {
     "duration": 0.032321,
     "end_time": "2024-09-16T09:54:45.988886",
     "exception": false,
     "start_time": "2024-09-16T09:54:45.956565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "\n",
    "def train(fold,model_type,model_name):\n",
    "    # Load the dataset with fold information\n",
    "    df = pd.read_csv(\"/kaggle/input/train-data/train_folds.csv\")\n",
    "    device = \"cuda\"  # Set the device to CUDA for GPU training\n",
    "    epochs = 50  # Number of training epochs\n",
    "    train_bs = 32  # Batch size for training\n",
    "    valid_bs = 16  # Batch size for validation\n",
    "\n",
    "    # Split data into training and validation based on the fold\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    # Convert pixel values to numpy arrays\n",
    "    train_pixels = df_train['pixels'].apply(lambda x: np.fromstring(x, sep=' ', dtype=np.float32))\n",
    "    train_targets = df_train['emotion'].values\n",
    "    valid_pixels = df_valid['pixels'].apply(lambda x: np.fromstring(x, sep=' ', dtype=np.float32))\n",
    "    valid_targets = df_valid['emotion'].values\n",
    "\n",
    "    # Define data augmentation and normalization for training\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    train_aug = A.Compose([\n",
    "        A.Resize(height=224, width=224),  # Resize images to 224x224\n",
    "        A.Normalize(mean=mean, std=std, max_pixel_value=255.0, always_apply=True),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15),\n",
    "        A.HorizontalFlip(p=0.5)  # Randomly flip images horizontally\n",
    "    ])\n",
    "\n",
    "    # Define data augmentation and normalization for validation\n",
    "    valid_aug = A.Compose([\n",
    "        A.Resize(height=224, width=224),  # Resize images to 224x224\n",
    "        A.Normalize(mean=mean, std=std, max_pixel_value=255.0, always_apply=True)\n",
    "    ])\n",
    "\n",
    "    # Create custom datasets with augmentations\n",
    "    train_dataset = CustomImageDataset(\n",
    "        pixel_arrays=train_pixels,\n",
    "        targets=train_targets,\n",
    "        resize=(224, 224),  # New size\n",
    "        augmentations=train_aug,\n",
    "    )\n",
    "    valid_dataset = CustomImageDataset(\n",
    "        pixel_arrays=valid_pixels,\n",
    "        targets=valid_targets,\n",
    "        resize=(224, 224),  # New size\n",
    "        augmentations=valid_aug,\n",
    "    )\n",
    "\n",
    "    # Create data loaders for training and validation\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True, num_workers=4\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=16, shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize the model, optimizer, and learning rate scheduler\n",
    "    model = model_type\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        patience=5,  # Number of epochs with no improvement to wait before reducing the learning rate\n",
    "        threshold=0.001,  # Minimum change to qualify as an improvement\n",
    "        mode=\"max\"  # Mode for monitoring metric improvement\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=5, mode=\"max\")  # Initialize early stopping\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train the model and get the average training loss\n",
    "        train_loss = Engine.train(train_loader, model, optimizer, device, scheduler=None, accumulation_steps=1, fp16=False)\n",
    "\n",
    "        # Evaluate the model and get predictions and validation loss\n",
    "        predictions, valid_loss = Engine.evaluate(valid_loader, model, device=device)\n",
    "        final_predictions = np.argmax(np.vstack(predictions), axis=1)\n",
    "        valid_targets = np.array(valid_targets)  # Ensure targets are in the right format\n",
    "    \n",
    "        # Calculate and print accuracy\n",
    "        accuracy = metrics.accuracy_score(valid_targets, final_predictions)\n",
    "        print(f\"Epoch = {epoch}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        # Update the learning rate based on validation accuracy\n",
    "        scheduler.step(accuracy)\n",
    "    \n",
    "        # Apply early stopping and saves the model \n",
    "        es(accuracy, model, model_path=(f\"model_fold_{model_name}_{fold}.bin\"))\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Create a DataFrame with out-of-fold predictions\n",
    "    oof_data = {\n",
    "        'id': df_valid.index,  # Use the index to map back to the original data\n",
    "        'true_emotion': valid_targets,\n",
    "        'pred_emotion': final_predictions,\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(oof_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d74010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:46.007264Z",
     "iopub.status.busy": "2024-09-16T09:54:46.006951Z",
     "iopub.status.idle": "2024-09-16T09:54:46.017564Z",
     "shell.execute_reply": "2024-09-16T09:54:46.016649Z"
    },
    "papermill": {
     "duration": 0.022236,
     "end_time": "2024-09-16T09:54:46.019521",
     "exception": false,
     "start_time": "2024-09-16T09:54:45.997285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import albumentations\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def predict(fold,model_type,model_name):\n",
    "    # Load the test dataset\n",
    "    df = pd.read_csv(\"/kaggle/input/emo-map-challenge/test_dataset.csv\")\n",
    "    device = \"cuda\"  # Set device to GPU\n",
    "    model_path = f\"model_fold_{model_name}_{fold}.bin\"  # Path to the trained model\n",
    "\n",
    "    # Convert pixel data from string to numpy arrays\n",
    "    test_pixels = df['pixels'].apply(lambda x: np.fromstring(x, sep=' ', dtype=np.float32))\n",
    "    \n",
    "    # Define the augmentation pipeline for test data\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    aug = albumentations.Compose([\n",
    "        albumentations.Resize(height=224, width=224),  # Resize images to 224x224\n",
    "        albumentations.Normalize(mean, std, max_pixel_value=255.0, always_apply=True)  # Normalize images\n",
    "    ])\n",
    "    \n",
    "    # Initialize dummy targets as placeholders\n",
    "    targets = np.zeros(len(df))\n",
    "    \n",
    "    # Create the test dataset with augmentations\n",
    "    test_dataset = CustomImageDataset(\n",
    "        pixel_arrays=test_pixels,\n",
    "        targets=targets,\n",
    "        resize=None,  # Resize is handled by augmentations\n",
    "        augmentations=aug,\n",
    "    )\n",
    "    \n",
    "    # Create a DataLoader for the test dataset\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=16, shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    # Initialize and load the model\n",
    "    model = model_type # Function to initialize the model (make sure it matches the one used during training)\n",
    "    model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
    "    model.to(device)  # Move the model to GPU\n",
    "\n",
    "    # Generate predictions using the test data\n",
    "    predictions = Engine.predict(test_loader, model, device=device)\n",
    "    predictions = np.vstack((predictions))  # Stack predictions into a single array\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f0c86c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:46.037931Z",
     "iopub.status.busy": "2024-09-16T09:54:46.037596Z",
     "iopub.status.idle": "2024-09-16T09:54:54.831145Z",
     "shell.execute_reply": "2024-09-16T09:54:54.829964Z"
    },
    "papermill": {
     "duration": 8.805717,
     "end_time": "2024-09-16T09:54:54.833915",
     "exception": false,
     "start_time": "2024-09-16T09:54:46.028198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100%|██████████| 171M/171M [00:01<00:00, 163MB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
      "100%|██████████| 230M/230M [00:04<00:00, 50.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_101 = ResNetForMultiClass(num_classes=7, model_type='resnet101', pretrained=True)\n",
    "model_150 = ResNetForMultiClass(num_classes=7, model_type='resnet152', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56932357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:54.863437Z",
     "iopub.status.busy": "2024-09-16T09:54:54.862668Z",
     "iopub.status.idle": "2024-09-16T09:54:54.880662Z",
     "shell.execute_reply": "2024-09-16T09:54:54.879625Z"
    },
    "papermill": {
     "duration": 0.034159,
     "end_time": "2024-09-16T09:54:54.882929",
     "exception": false,
     "start_time": "2024-09-16T09:54:54.848770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Discriminative Feature Attention Network (applied after major blocks)\n",
    "#Model 3 is also a Resnet34, But modified with the Discrminative feature selection attenation Mechanism, \n",
    "#So I'll the that model from scratch\n",
    "class DiscriminativeFeatureAttentionNetwork(nn.Module):\n",
    "    def __init__(self, input_channels, reduction_ratio=16):\n",
    "        super(DiscriminativeFeatureAttentionNetwork, self).__init__()\n",
    "        \n",
    "        # Global average pooling layer\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layers to compute attention weights\n",
    "        self.fc1 = nn.Linear(input_channels, input_channels // reduction_ratio)\n",
    "        self.fc2 = nn.Linear(input_channels // reduction_ratio, input_channels)\n",
    "        \n",
    "        # Activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x has shape: (batch_size, channels, height, width)\n",
    "        \n",
    "        # Step 1: Global average pooling to get (batch_size, channels)\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_pooled = self.global_avg_pool(x).view(b, c)\n",
    "        \n",
    "        # Step 2: Pass through the fully connected layers\n",
    "        fc1_out = self.relu(self.fc1(avg_pooled))\n",
    "        fc2_out = self.sigmoid(self.fc2(fc1_out))\n",
    "        \n",
    "        # Step 3: Reshape the attention weights to (batch_size, channels, 1, 1)\n",
    "        attention_weights = fc2_out.view(b, c, 1, 1)\n",
    "        \n",
    "        # Step 4: Apply attention weights to input feature maps\n",
    "        out = x * attention_weights  # Element-wise multiplication to focus on discriminative features\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ResNet101 with integrated Discriminative Feature Attention for multi-class classification\n",
    "class ResNet34ForMultiClassWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes=7, pretrained=True):\n",
    "        super(ResNet34ForMultiClassWithAttention, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet101 model\n",
    "        self.base_model = models.resnet34(pretrained=pretrained)\n",
    "        \n",
    "        # Modify the fully connected layer to match the number of classes\n",
    "        self.base_model.fc = nn.Linear(\n",
    "            in_features=self.base_model.fc.in_features,\n",
    "            out_features=num_classes\n",
    "        )\n",
    "        \n",
    "        # Add the Discriminative Feature Attention after layer3 of ResNet101\n",
    "        self.discriminative_attention = DiscriminativeFeatureAttentionNetwork(input_channels=256)  # 1024 channels in layer3 of ResNet101\n",
    "\n",
    "    def forward(self, image, targets=None):\n",
    "        batch_size = image.size(0)\n",
    "        \n",
    "        # Forward pass through the base model's initial layers\n",
    "        x = self.base_model.conv1(image)\n",
    "        x = self.base_model.bn1(x)\n",
    "        x = self.base_model.relu(x)\n",
    "        x = self.base_model.maxpool(x)\n",
    "\n",
    "        # Forward through ResNet's layer1, layer2, and layer3\n",
    "        x = self.base_model.layer1(x)\n",
    "        x = self.base_model.layer2(x)\n",
    "        x = self.base_model.layer3(x)\n",
    "        \n",
    "        # Apply Discriminative Feature Attention after layer3\n",
    "        x = self.discriminative_attention(x)\n",
    "        \n",
    "        # Forward through ResNet's layer4\n",
    "        x = self.base_model.layer4(x)\n",
    "        \n",
    "        # Global average pooling before the final classification layer\n",
    "        x = self.base_model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Get the raw logits from the fully connected layer\n",
    "        out = self.base_model.fc(x)\n",
    "        \n",
    "        # If targets are provided, calculate the loss\n",
    "        if targets is not None:\n",
    "            loss = nn.CrossEntropyLoss()(out, targets)\n",
    "            return out, loss\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0edb083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:54.911304Z",
     "iopub.status.busy": "2024-09-16T09:54:54.910980Z",
     "iopub.status.idle": "2024-09-16T09:54:58.231852Z",
     "shell.execute_reply": "2024-09-16T09:54:58.230699Z"
    },
    "papermill": {
     "duration": 3.338044,
     "end_time": "2024-09-16T09:54:58.234380",
     "exception": false,
     "start_time": "2024-09-16T09:54:54.896336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:00<00:00, 172MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_1 = ResNetForMultiClass(num_classes=7, model_type='resnet152', pretrained=True)\n",
    "model_2 = ResNetForMultiClass(num_classes=7, model_type='resnet101', pretrained=True)\n",
    "model_3 = ResNet34ForMultiClassWithAttention(num_classes = 7,pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb1e5301",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T09:54:58.265141Z",
     "iopub.status.busy": "2024-09-16T09:54:58.264149Z",
     "iopub.status.idle": "2024-09-16T11:55:39.260910Z",
     "shell.execute_reply": "2024-09-16T11:55:39.259429Z"
    },
    "papermill": {
     "duration": 7241.014351,
     "end_time": "2024-09-16T11:55:39.263344",
     "exception": false,
     "start_time": "2024-09-16T09:54:58.248993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=1.44]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.04it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.4790\n",
      "Validation score improved (-inf --> 0.479). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=1.11]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.26it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.5350\n",
      "Validation score improved (0.479 --> 0.535). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.975]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.21it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.5380\n",
      "Validation score improved (0.535 --> 0.538). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.816]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.19it/s, loss=1.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.5640\n",
      "Validation score improved (0.538 --> 0.564). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.695]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.22it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.5810\n",
      "Validation score improved (0.564 --> 0.581). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.581]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.22it/s, loss=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.5530\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.465]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.21it/s, loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.5380\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.394]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.20it/s, loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, Accuracy = 0.5570\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.345]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.24it/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, Accuracy = 0.5680\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.287]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.19it/s, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9, Accuracy = 0.5420\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.626]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.21it/s, loss=0.229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9460\n",
      "Validation score improved (-inf --> 0.946). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.389]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.21it/s, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9180\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.311]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.16it/s, loss=0.295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.8970\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.235]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.22it/s, loss=0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.8940\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.242]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.22it/s, loss=0.405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.8690\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.224]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.18it/s, loss=0.407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.8440\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.325]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.24it/s, loss=0.121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9650\n",
      "Validation score improved (-inf --> 0.965). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.237]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.23it/s, loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9630\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.182]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.20it/s, loss=0.136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9590\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.203]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.19it/s, loss=0.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9480\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.16]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.21it/s, loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9350\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.171]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.20it/s, loss=0.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9130\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.233]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.22it/s, loss=0.0767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9710\n",
      "Validation score improved (-inf --> 0.971). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.172]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.23it/s, loss=0.0595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9840\n",
      "Validation score improved (0.971 --> 0.984). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.125]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.25it/s, loss=0.0915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9690\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.162]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.18it/s, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9230\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.17]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.21it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9460\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.118]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.23it/s, loss=0.0991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9610\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.114]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.20it/s, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.9330\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.201]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.22it/s, loss=0.054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9850\n",
      "Validation score improved (-inf --> 0.985). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.105]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.23it/s, loss=0.0686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9800\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.124]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.20it/s, loss=0.0749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9760\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.132]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.23it/s, loss=0.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9640\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.122]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.23it/s, loss=0.0901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9670\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:29<00:00,  1.40it/s, loss=0.108]\n",
      "100%|██████████| 63/63 [00:07<00:00,  8.22it/s, loss=0.107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9620\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.95it/s, loss=1.42]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.45it/s, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.5430\n",
      "Validation score improved (-inf --> 0.543). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=1.12]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.47it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.5410\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.948]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.40it/s, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.5810\n",
      "Validation score improved (0.543 --> 0.581). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.818]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.44it/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.5350\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.678]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.35it/s, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.5460\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.578]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.27it/s, loss=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.5370\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.506]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.45it/s, loss=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.5820\n",
      "Validation score improved (0.581 --> 0.582). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.426]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.42it/s, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, Accuracy = 0.5670\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.352]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.37it/s, loss=1.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, Accuracy = 0.5620\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.312]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.45it/s, loss=1.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9, Accuracy = 0.5690\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.267]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.38it/s, loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10, Accuracy = 0.5510\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.281]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.44it/s, loss=1.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11, Accuracy = 0.5720\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.59]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.43it/s, loss=0.221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9320\n",
      "Validation score improved (-inf --> 0.932). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.376]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.45it/s, loss=0.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9230\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.313]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.33it/s, loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.8880\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.259]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.35it/s, loss=0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.8990\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.249]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.39it/s, loss=0.353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.8730\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.206]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.34it/s, loss=0.549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.8170\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.29]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.41it/s, loss=0.0704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9780\n",
      "Validation score improved (-inf --> 0.978). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.195]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.31it/s, loss=0.0765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9810\n",
      "Validation score improved (0.978 --> 0.981). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.206]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.43it/s, loss=0.105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9670\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.173]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.46it/s, loss=0.167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9450\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.192]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.46it/s, loss=0.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9370\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.162]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.46it/s, loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9600\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.149]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.47it/s, loss=0.205]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.9310\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.228]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.32it/s, loss=0.0891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9680\n",
      "Validation score improved (-inf --> 0.968). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.164]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.41it/s, loss=0.0934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9730\n",
      "Validation score improved (0.968 --> 0.973). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.149]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.44it/s, loss=0.124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9530\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.97it/s, loss=0.154]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.49it/s, loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9590\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.142]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.47it/s, loss=0.143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9510\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.141]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.46it/s, loss=0.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9500\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.97it/s, loss=0.15]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.46it/s, loss=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.9430\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.198]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.43it/s, loss=0.05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9870\n",
      "Validation score improved (-inf --> 0.987). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.102]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.38it/s, loss=0.0525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9850\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.97it/s, loss=0.122]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.41it/s, loss=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9670\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.123]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.30it/s, loss=0.114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9650\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.119]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.47it/s, loss=0.119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9550\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:03<00:00,  1.96it/s, loss=0.104]\n",
      "100%|██████████| 63/63 [00:05<00:00, 11.42it/s, loss=0.118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9610\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.81it/s, loss=1.45]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.32it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.4880\n",
      "Validation score improved (-inf --> 0.488). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=1.11]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.48it/s, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.5280\n",
      "Validation score improved (0.488 --> 0.528). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.88it/s, loss=0.9]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.40it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.5260\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.751]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.16it/s, loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.5160\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.85it/s, loss=0.621]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.22it/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.5420\n",
      "Validation score improved (0.528 --> 0.542). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.88it/s, loss=0.516]\n",
      "100%|██████████| 63/63 [00:02<00:00, 23.56it/s, loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.5580\n",
      "Validation score improved (0.542 --> 0.558). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.436]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.33it/s, loss=1.52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.5560\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.35]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.35it/s, loss=1.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, Accuracy = 0.5720\n",
      "Validation score improved (0.558 --> 0.572). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.29]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.19it/s, loss=1.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, Accuracy = 0.5560\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.25]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.17it/s, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9, Accuracy = 0.5570\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.229]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.28it/s, loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10, Accuracy = 0.5540\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.218]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.05it/s, loss=1.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11, Accuracy = 0.5800\n",
      "Validation score improved (0.572 --> 0.58). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.171]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.91it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12, Accuracy = 0.5570\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.82it/s, loss=0.177]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.78it/s, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13, Accuracy = 0.5710\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.83it/s, loss=0.164]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.23it/s, loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14, Accuracy = 0.5410\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.171]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.25it/s, loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15, Accuracy = 0.5870\n",
      "Validation score improved (0.58 --> 0.587). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.14]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.92it/s, loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 16, Accuracy = 0.5870\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.133]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.12it/s, loss=1.83]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 17, Accuracy = 0.5730\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.82it/s, loss=0.122]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.19it/s, loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 18, Accuracy = 0.5740\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.87it/s, loss=0.131]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.86it/s, loss=2.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 19, Accuracy = 0.5700\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.85it/s, loss=0.115]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.35it/s, loss=1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 20, Accuracy = 0.5650\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.83it/s, loss=0.56]\n",
      "100%|██████████| 63/63 [00:02<00:00, 24.89it/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9580\n",
      "Validation score improved (-inf --> 0.958). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.85it/s, loss=0.3]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.42it/s, loss=0.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9690\n",
      "Validation score improved (0.958 --> 0.969). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.236]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.97it/s, loss=0.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9540\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.85it/s, loss=0.174]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.30it/s, loss=0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9600\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.147]\n",
      "100%|██████████| 63/63 [00:02<00:00, 23.80it/s, loss=0.183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9470\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.165]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.92it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9140\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.80it/s, loss=0.133]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.13it/s, loss=0.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.9340\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.208]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.42it/s, loss=0.0365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9880\n",
      "Validation score improved (-inf --> 0.988). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.83it/s, loss=0.157]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.36it/s, loss=0.0454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9890\n",
      "Validation score improved (0.988 --> 0.989). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.87it/s, loss=0.134]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.37it/s, loss=0.036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9890\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.85it/s, loss=0.126]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.10it/s, loss=0.0585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9850\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.85it/s, loss=0.118]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.06it/s, loss=0.0715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9750\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.85it/s, loss=0.112]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.08it/s, loss=0.0708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9780\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.136]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.12it/s, loss=0.151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.9480\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.83it/s, loss=0.152]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.25it/s, loss=0.0565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9830\n",
      "Validation score improved (-inf --> 0.983). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.13]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.08it/s, loss=0.0359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9900\n",
      "Validation score improved (0.983 --> 0.99). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.117]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.24it/s, loss=0.0797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9710\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.0969]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.43it/s, loss=0.0725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9740\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.85it/s, loss=0.0815]\n",
      "100%|██████████| 63/63 [00:02<00:00, 25.95it/s, loss=0.057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9780\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.107]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.34it/s, loss=0.0932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9700\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.108]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.18it/s, loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.9500\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.131]\n",
      "100%|██████████| 63/63 [00:02<00:00, 24.84it/s, loss=0.0478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9870\n",
      "Validation score improved (-inf --> 0.987). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.12]\n",
      "100%|██████████| 63/63 [00:02<00:00, 24.58it/s, loss=0.0775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9710\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.83it/s, loss=0.0917]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.19it/s, loss=0.0427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9870\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.84it/s, loss=0.101]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.25it/s, loss=0.0674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9790\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.86it/s, loss=0.076]\n",
      "100%|██████████| 63/63 [00:02<00:00, 23.39it/s, loss=0.0569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9780\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:21<00:00,  5.74it/s, loss=0.0887]\n",
      "100%|██████████| 63/63 [00:02<00:00, 26.26it/s, loss=0.093]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9670\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from warnings import filterwarnings\n",
    "model_types = [model_1, model_2, model_3]\n",
    "model_names = ['RES152','RES101','RES32D']\n",
    "for model_type, model_name in zip(model_types, model_names):\n",
    "    X = np.empty((0, 3))\n",
    "    for fold in range(5):\n",
    "        # Train the model for the current fold and model type\n",
    "        df = train(fold, model_type,model_name)\n",
    "        df.to_csv(f\"oof{model_name}-{fold}.csv\", index=False) \n",
    "        X = np.concatenate((X, df.values))  \n",
    "    df_combined = pd.DataFrame(X, columns=['id', 'true_emotion', 'pred_emotion']) \n",
    "    df_combined.to_csv(f'{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86f04dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T11:55:47.148134Z",
     "iopub.status.busy": "2024-09-16T11:55:47.147665Z",
     "iopub.status.idle": "2024-09-16T11:59:28.995798Z",
     "shell.execute_reply": "2024-09-16T11:59:28.994510Z"
    },
    "papermill": {
     "duration": 225.827642,
     "end_time": "2024-09-16T11:59:28.998447",
     "exception": false,
     "start_time": "2024-09-16T11:55:43.170805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:18<00:00,  8.38it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:18<00:00,  8.28it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:18<00:00,  8.36it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:18<00:00,  8.40it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:18<00:00,  8.40it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:13<00:00, 11.74it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:13<00:00, 11.70it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:13<00:00, 11.73it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:13<00:00, 11.70it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:13<00:00, 11.68it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:05<00:00, 27.77it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:05<00:00, 28.12it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:05<00:00, 28.09it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:05<00:00, 27.83it/s]\n",
      "/tmp/ipykernel_22/721335947.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))  # Load the trained model weights\n",
      "100%|██████████| 157/157 [00:05<00:00, 28.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from warnings import filterwarnings\n",
    "dfpred = pd.DataFrame()\n",
    "for model_type, model_name in zip(model_types, model_names):\n",
    "    folds = range(5)\n",
    "    predictions_list = []\n",
    "    for fold in folds:\n",
    "        predictions = predict(fold,model_type,model_name)  # Get predictions for the current fold\n",
    "        predictions_list.append(predictions)  # Store predictions\n",
    "    p = np.mean(predictions_list, axis=0)\n",
    "    pred = np.argmax(p, axis=1)\n",
    "    dfpred[model_name] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ede74ad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T11:59:37.668410Z",
     "iopub.status.busy": "2024-09-16T11:59:37.667975Z",
     "iopub.status.idle": "2024-09-16T11:59:37.675578Z",
     "shell.execute_reply": "2024-09-16T11:59:37.674700Z"
    },
    "papermill": {
     "duration": 4.283156,
     "end_time": "2024-09-16T11:59:37.677710",
     "exception": false,
     "start_time": "2024-09-16T11:59:33.394554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  3 Models Ensembled by taking mode\n",
    "from scipy import stats\n",
    "def calculate_mode_for_rows(df):\n",
    "    def row_mode(row):\n",
    "        # Check if col1 is 1, then mode should be 1\n",
    "        if row['RES152'] == 1:\n",
    "            return 1\n",
    "        \n",
    "        # Create a list of values to calculate mode\n",
    "        values = [row['RES152'], row['RES101'], row['RES32D']]\n",
    "        \n",
    "        # Calculate mode\n",
    "        mode = pd.Series(values).mode()\n",
    "        \n",
    "        # Return mode if it exists\n",
    "        if not mode.empty:\n",
    "            return mode.iloc[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply the row_mode function to each row\n",
    "    df['mode'] = df.apply(row_mode, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d414bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T11:59:46.084595Z",
     "iopub.status.busy": "2024-09-16T11:59:46.083637Z",
     "iopub.status.idle": "2024-09-16T11:59:46.791253Z",
     "shell.execute_reply": "2024-09-16T11:59:46.790126Z"
    },
    "papermill": {
     "duration": 4.942928,
     "end_time": "2024-09-16T11:59:46.793655",
     "exception": false,
     "start_time": "2024-09-16T11:59:41.850727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfpred = calculate_mode_for_rows(dfpred)\n",
    "dfsy = pd.read_csv('/kaggle/input/emo-map-challenge/sample_submission.csv')\n",
    "dfsy['emotion']=dfpred['mode']\n",
    "dfsy.to_csv('finalresult.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16137154",
   "metadata": {
    "papermill": {
     "duration": 4.183969,
     "end_time": "2024-09-16T11:59:55.113922",
     "exception": false,
     "start_time": "2024-09-16T11:59:50.929953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# section 2\n",
    "Class imbalance is the problem that need attention, so I have tried various loss functions,but they gave almost same accuracy, here are the loss functions tried"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d69292",
   "metadata": {
    "papermill": {
     "duration": 4.298664,
     "end_time": "2024-09-16T12:00:03.630710",
     "exception": false,
     "start_time": "2024-09-16T11:59:59.332046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dynamic attention loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ec5e4e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T12:00:12.029861Z",
     "iopub.status.busy": "2024-09-16T12:00:12.029361Z",
     "iopub.status.idle": "2024-09-16T12:00:12.039566Z",
     "shell.execute_reply": "2024-09-16T12:00:12.038681Z"
    },
    "papermill": {
     "duration": 4.252845,
     "end_time": "2024-09-16T12:00:12.041714",
     "exception": false,
     "start_time": "2024-09-16T12:00:07.788869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(predictions, targets, num_classes, device):\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes, device=device)\n",
    "    pred_labels = predictions.argmax(dim=1)\n",
    "    for t, p in zip(targets, pred_labels):\n",
    "        confusion_matrix[t, p] += 1\n",
    "    confusion_matrix = confusion_matrix / confusion_matrix.sum(dim=1, keepdim=True)\n",
    "    confusion_matrix[confusion_matrix != confusion_matrix] = 0 \n",
    "    return confusion_matrix\n",
    "class DynamicAttentionLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, num_classes=10):\n",
    "        super(DynamicAttentionLoss, self).__init__()\n",
    "        self.alpha = alpha \n",
    "        self.num_classes = num_classes \n",
    "    def forward(self, predictions, targets, confusion_matrix):\n",
    "        one_hot_labels = F.one_hot(targets, num_classes=self.num_classes).float()\n",
    "        soft_labels = (1 - self.alpha) * one_hot_labels + self.alpha * confusion_matrix[targets]\n",
    "        log_probs = F.log_softmax(predictions, dim=1)\n",
    "        loss = -torch.sum(soft_labels * log_probs, dim=1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83992cdd",
   "metadata": {
    "papermill": {
     "duration": 4.249768,
     "end_time": "2024-09-16T12:00:20.441116",
     "exception": false,
     "start_time": "2024-09-16T12:00:16.191348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "430ac0be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T12:00:28.872833Z",
     "iopub.status.busy": "2024-09-16T12:00:28.872399Z",
     "iopub.status.idle": "2024-09-16T12:00:28.881480Z",
     "shell.execute_reply": "2024-09-16T12:00:28.880567Z"
    },
    "papermill": {
     "duration": 4.261957,
     "end_time": "2024-09-16T12:00:28.883508",
     "exception": false,
     "start_time": "2024-09-16T12:00:24.621551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from NaN-loss errors\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: [batch_size, num_classes]\n",
    "        target: [batch_size] (true class indices, not one-hot)\n",
    "        \"\"\"\n",
    "        # Compute softmax over logits to get the class probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Gather the probabilities of the true class (index-based selection)\n",
    "        probs_target_class = probs.gather(1, target.unsqueeze(1)).squeeze(1) + self.epsilon\n",
    "        \n",
    "        # Compute the log of the selected probabilities\n",
    "        log_pt = torch.log(probs_target_class)\n",
    "        \n",
    "        # Calculate the focal loss (focuses more on hard examples)\n",
    "        focal_loss = -1 * self.alpha * (1 - probs_target_class) ** self.gamma * log_pt\n",
    "        \n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a772119",
   "metadata": {
    "papermill": {
     "duration": 4.253137,
     "end_time": "2024-09-16T12:00:37.442045",
     "exception": false,
     "start_time": "2024-09-16T12:00:33.188908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Centre Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a146a0",
   "metadata": {
    "papermill": {
     "duration": 4.279352,
     "end_time": "2024-09-16T12:00:45.922075",
     "exception": false,
     "start_time": "2024-09-16T12:00:41.642723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This Loss function mainly focuses on discriminative features where the data is less for imbalanced classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3db1e70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T12:00:54.290141Z",
     "iopub.status.busy": "2024-09-16T12:00:54.289216Z",
     "iopub.status.idle": "2024-09-16T12:00:54.301701Z",
     "shell.execute_reply": "2024-09-16T12:00:54.300748Z"
    },
    "papermill": {
     "duration": 4.162484,
     "end_time": "2024-09-16T12:00:54.303936",
     "exception": false,
     "start_time": "2024-09-16T12:00:50.141452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"Center loss.\n",
    "    \n",
    "    Reference:\n",
    "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=7, feat_dim=2048, use_gpu=True):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cead9f",
   "metadata": {
    "papermill": {
     "duration": 4.182016,
     "end_time": "2024-09-16T12:01:02.709033",
     "exception": false,
     "start_time": "2024-09-16T12:00:58.527017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In order to capitalise on Centre loss one should extract the deep features which are useful to descriminate the features, here is the typical fextraction of deep features to train the model,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "226c6fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T12:01:11.180327Z",
     "iopub.status.busy": "2024-09-16T12:01:11.179918Z",
     "iopub.status.idle": "2024-09-16T12:01:11.216599Z",
     "shell.execute_reply": "2024-09-16T12:01:11.215576Z"
    },
    "papermill": {
     "duration": 4.272916,
     "end_time": "2024-09-16T12:01:11.218926",
     "exception": false,
     "start_time": "2024-09-16T12:01:06.946010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x,targets = None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # To save pre-final layer's features\n",
    "\n",
    "        feat = self.avgpool(self.layer4(x))\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        x = self.fc(feat)\n",
    "        if targets is not None:\n",
    "            loss = self.loss_fn(x,targets)\n",
    "            return x, loss\n",
    "        \n",
    "        return feat, x    #Extracted deep-features represented as 'feat'\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9530234,
     "sourceId": 84797,
     "sourceType": "competition"
    },
    {
     "datasetId": 5713481,
     "sourceId": 9409375,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7623.194247,
   "end_time": "2024-09-16T12:01:18.428478",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-16T09:54:15.234231",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
