{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "290135fb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-14T05:53:58.994863Z",
     "iopub.status.busy": "2024-09-14T05:53:58.994182Z",
     "iopub.status.idle": "2024-09-14T05:53:59.842475Z",
     "shell.execute_reply": "2024-09-14T05:53:59.841592Z"
    },
    "papermill": {
     "duration": 0.857536,
     "end_time": "2024-09-14T05:53:59.844770",
     "exception": false,
     "start_time": "2024-09-14T05:53:58.987234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/kiolpujy/class4.csv\n",
      "/kaggle/input/kiolpujy/class3.csv\n",
      "/kaggle/input/kiolpujy/class1.csv\n",
      "/kaggle/input/kiolpujy/class6.csv\n",
      "/kaggle/input/kiolpujy/class0.csv\n",
      "/kaggle/input/kiolpujy/class5.csv\n",
      "/kaggle/input/kiolpujy/class2.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01833810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:53:59.855624Z",
     "iopub.status.busy": "2024-09-14T05:53:59.855219Z",
     "iopub.status.idle": "2024-09-14T05:54:00.920667Z",
     "shell.execute_reply": "2024-09-14T05:54:00.919750Z"
    },
    "papermill": {
     "duration": 1.073369,
     "end_time": "2024-09-14T05:54:00.922966",
     "exception": false,
     "start_time": "2024-09-14T05:53:59.849597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'pixels', 'class_1'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/kiolpujy/class1.csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5025ee63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:00.934564Z",
     "iopub.status.busy": "2024-09-14T05:54:00.934243Z",
     "iopub.status.idle": "2024-09-14T05:54:15.374777Z",
     "shell.execute_reply": "2024-09-14T05:54:15.373605Z"
    },
    "papermill": {
     "duration": 14.44867,
     "end_time": "2024-09-14T05:54:15.376868",
     "exception": false,
     "start_time": "2024-09-14T05:54:00.928198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wtfml\r\n",
      "  Downloading wtfml-0.0.3-py3-none-any.whl.metadata (808 bytes)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.10/site-packages (from wtfml) (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->wtfml) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->wtfml) (1.14.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->wtfml) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.1->wtfml) (3.5.0)\r\n",
      "Downloading wtfml-0.0.3-py3-none-any.whl (10 kB)\r\n",
      "Installing collected packages: wtfml\r\n",
      "Successfully installed wtfml-0.0.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wtfml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb15af7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:15.389233Z",
     "iopub.status.busy": "2024-09-14T05:54:15.388878Z",
     "iopub.status.idle": "2024-09-14T05:54:21.783529Z",
     "shell.execute_reply": "2024-09-14T05:54:21.782687Z"
    },
    "papermill": {
     "duration": 6.40335,
     "end_time": "2024-09-14T05:54:21.785856",
     "exception": false,
     "start_time": "2024-09-14T05:54:15.382506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.15 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import albumentations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from wtfml.utils import EarlyStopping\n",
    "from wtfml.engine import Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8118c180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:21.798918Z",
     "iopub.status.busy": "2024-09-14T05:54:21.798441Z",
     "iopub.status.idle": "2024-09-14T05:54:21.832814Z",
     "shell.execute_reply": "2024-09-14T05:54:21.831992Z"
    },
    "papermill": {
     "duration": 0.043147,
     "end_time": "2024-09-14T05:54:21.834810",
     "exception": false,
     "start_time": "2024-09-14T05:54:21.791663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x,targets = None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # To save pre-final layer's features\n",
    "\n",
    "        feat = self.avgpool(self.layer4(x))\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        x = self.fc(feat)\n",
    "        if targets is not None:\n",
    "            loss = self.loss_fn(x,targets)\n",
    "            return x, loss\n",
    "        \n",
    "        return feat, x    #Extracted deep-features represented as 'feat'\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a5950a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:21.847546Z",
     "iopub.status.busy": "2024-09-14T05:54:21.846854Z",
     "iopub.status.idle": "2024-09-14T05:54:21.851591Z",
     "shell.execute_reply": "2024-09-14T05:54:21.850665Z"
    },
    "papermill": {
     "duration": 0.012691,
     "end_time": "2024-09-14T05:54:21.853397",
     "exception": false,
     "start_time": "2024-09-14T05:54:21.840706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(num_classes):\n",
    "    model = resnet152(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs,num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a893b493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:21.865275Z",
     "iopub.status.busy": "2024-09-14T05:54:21.864843Z",
     "iopub.status.idle": "2024-09-14T05:54:21.890578Z",
     "shell.execute_reply": "2024-09-14T05:54:21.889752Z"
    },
    "papermill": {
     "duration": 0.03381,
     "end_time": "2024-09-14T05:54:21.892403",
     "exception": false,
     "start_time": "2024-09-14T05:54:21.858593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, pixel_arrays, targets, resize=None, augmentations=None):\n",
    "        self.pixel_arrays = pixel_arrays\n",
    "        self.targets = targets\n",
    "        self.resize = resize\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pixel_arrays)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the 1D array to a 2D grayscale image\n",
    "        image = np.array(self.pixel_arrays[idx], dtype=np.float32).reshape(48, 48)\n",
    "        \n",
    "        # Resize image if needed\n",
    "        if self.resize:\n",
    "            image = cv2.resize(image, self.resize)\n",
    "\n",
    "        # Expand dimensions to (48, 48, 1) and then convert to (new_size, new_size, 3) for RGB\n",
    "        image = np.expand_dims(image, axis=-1)\n",
    "        image = np.repeat(image, 3, axis=-1)  # Convert to RGB\n",
    "\n",
    "        # Apply augmentations if specified\n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        # Convert image to PyTorch tensor and permute to (3, new_size, new_size) format\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)  # For RGB, shape becomes (3, new_size, new_size)\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "        return image, target\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "class Engine:\n",
    "    @staticmethod\n",
    "    def train(data_loader, model, optimizer, device, centerloss, nllloss, loss_weight=0.001, scheduler=None, accumulation_steps=1, fp16=False):\n",
    "        model.train()\n",
    "        losses = AverageMeter()\n",
    "        scaler = torch.cuda.amp.GradScaler() if fp16 else None\n",
    "\n",
    "        if accumulation_steps > 1:\n",
    "            optimizer[0].zero_grad()  # Zero grad for model optimizer\n",
    "            optimizer[1].zero_grad()  # Zero grad for center loss optimizer\n",
    "\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "\n",
    "        for batch_idx, (images, targets) in enumerate(tk0):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer[0].zero_grad()\n",
    "            optimizer[1].zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                if fp16:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        feats, outputs = model(images)\n",
    "                        loss = centerloss(feats, targets) * loss_weight + nllloss(outputs, targets)\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    feats, outputs = model(images)\n",
    "                    loss = centerloss(feats, targets)* loss_weight + nllloss(outputs, targets)\n",
    "                    loss.backward()\n",
    "\n",
    "                # Step the optimizers after accumulating gradients\n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    if fp16:\n",
    "                        scaler.step(optimizer[0])\n",
    "                        scaler.step(optimizer[1])\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        optimizer[0].step()  # Update model parameters\n",
    "                        optimizer[1].step()  # Update center loss parameters\n",
    "\n",
    "                    if scheduler:\n",
    "                        scheduler.step()\n",
    "\n",
    "                    optimizer[0].zero_grad()\n",
    "                    optimizer[1].zero_grad()\n",
    "\n",
    "            # Update average loss\n",
    "            losses.update(loss.item(), data_loader.batch_size)\n",
    "            tk0.set_postfix(loss=losses.avg)\n",
    "\n",
    "        return losses.avg\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(data_loader, model, device, use_tpu=False):\n",
    "        losses = AverageMeter()\n",
    "        final_predictions = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                images, targets = data  # Adjust if your data format is different\n",
    "\n",
    "                # Move tensors to device\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                predictions, loss = model(images, targets)\n",
    "                predictions = predictions.cpu()\n",
    "                losses.update(loss.item(), images.size(0))\n",
    "                final_predictions.append(predictions)\n",
    "                tk0.set_postfix(loss=losses.avg)\n",
    "\n",
    "        # Concatenate all predictions and convert to NumPy array\n",
    "        final_predictions = torch.cat(final_predictions).numpy()\n",
    "        return final_predictions, losses.avg\n",
    "    def predict(data_loader, model, device, use_tpu=False):\n",
    "        model.eval()\n",
    "        final_predictions = []\n",
    "        with torch.no_grad():\n",
    "            tk0 = tqdm(data_loader, total=len(data_loader), disable=use_tpu)\n",
    "            for b_idx, data in enumerate(tk0):\n",
    "                inputs, _ = data  # Unpack data\n",
    "                inputs = inputs.to(device)\n",
    "                predictions = model(inputs) \n",
    "                predictions = F.softmax(predictions,dim = 1)# Assume model returns only predictions\n",
    "                final_predictions.append(predictions.cpu())\n",
    "                tk0.set_postfix()\n",
    "        return torch.cat(final_predictions).numpy()\n",
    "        # Concatenate all predictions and convert to numpy array\n",
    "        return torch.cat(final_predictions).numpy()\n",
    "class AverageMeter():\n",
    "        def __init__(self):\n",
    "            self.reset()\n",
    "\n",
    "        def reset(self):\n",
    "            self.val = 0\n",
    "            self.avg = 0\n",
    "            self.sum = 0\n",
    "            self.count = 0\n",
    "\n",
    "        def update(self, val, n=1):\n",
    "            self.val = val\n",
    "            self.sum += val * n\n",
    "            self.count += n\n",
    "            self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "923f1fc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:21.904446Z",
     "iopub.status.busy": "2024-09-14T05:54:21.904167Z",
     "iopub.status.idle": "2024-09-14T05:54:21.913828Z",
     "shell.execute_reply": "2024-09-14T05:54:21.913011Z"
    },
    "papermill": {
     "duration": 0.017926,
     "end_time": "2024-09-14T05:54:21.915749",
     "exception": false,
     "start_time": "2024-09-14T05:54:21.897823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    def __init__(self, num_classes=7, feat_dim=512, use_gpu=True):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33a01e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:21.927239Z",
     "iopub.status.busy": "2024-09-14T05:54:21.926949Z",
     "iopub.status.idle": "2024-09-14T05:54:22.276362Z",
     "shell.execute_reply": "2024-09-14T05:54:22.275555Z"
    },
    "papermill": {
     "duration": 0.357689,
     "end_time": "2024-09-14T05:54:22.278647",
     "exception": false,
     "start_time": "2024-09-14T05:54:21.920958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "def apply_smote_to_dataframe(df, pixel_column, label_column, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTE to a DataFrame containing pixel data in string format and class labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the pixel data and labels.\n",
    "    - pixel_column (str): Column name for the pixel strings.\n",
    "    - label_column (str): Column name for the labels.\n",
    "    - random_state (int): Seed for random number generation.\n",
    "    \n",
    "    Returns:\n",
    "    - df_resampled (pd.DataFrame): DataFrame with resampled pixel data and labels.\n",
    "    \"\"\"\n",
    "    # Extract pixel strings and labels from the DataFrame\n",
    "    X_strings = df[pixel_column]\n",
    "    y = df[label_column]\n",
    "\n",
    "    # Convert the pixel strings into flattened arrays (2304 pixels per image)\n",
    "    X_flattened = np.array([np.array(x.split(), dtype='float32') for x in X_strings])\n",
    "\n",
    "    # Initialize SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "\n",
    "    # Apply SMOTE to the flattened arrays\n",
    "    X_resampled_flattened, y_resampled = smote.fit_resample(X_flattened, y)\n",
    "\n",
    "    # Convert the resampled arrays back into space-separated strings\n",
    "    X_resampled_strings = [' '.join(map(str, x)) for x in X_resampled_flattened]\n",
    "\n",
    "    # Create a new DataFrame with resampled pixel strings and labels\n",
    "    df_resampled = pd.DataFrame({\n",
    "        pixel_column: X_resampled_strings,\n",
    "        label_column: y_resampled\n",
    "    })\n",
    "\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6678fea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:22.291051Z",
     "iopub.status.busy": "2024-09-14T05:54:22.290589Z",
     "iopub.status.idle": "2024-09-14T05:54:22.310998Z",
     "shell.execute_reply": "2024-09-14T05:54:22.310257Z"
    },
    "papermill": {
     "duration": 0.028758,
     "end_time": "2024-09-14T05:54:22.312896",
     "exception": false,
     "start_time": "2024-09-14T05:54:22.284138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "def train(fold):\n",
    "    df = pd.read_csv(f\"/kaggle/input/kiolpujy/class{fold}.csv\")\n",
    "    df_train, df_valid = train_test_split(df, test_size=0.2, random_state=42, stratify=df[f'class_{fold}'])\n",
    "    pixel_column = 'pixels'\n",
    "    label_column = f'class_{fold}'\n",
    "    df_train = apply_smote_to_dataframe(df_train, pixel_column, label_column, random_state=42)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_valid = df_valid.reset_index(drop=True)\n",
    "    device = \"cuda\"\n",
    "    epochs = 50\n",
    "    train_bs = 32\n",
    "    valid_bs = 16\n",
    "    train_pixels = df_train['pixels'].apply(lambda x: np.fromstring(x, sep=' ', dtype=np.float32))\n",
    "    train_targets = df_train[f'class_{fold}'].values\n",
    "\n",
    "    valid_pixels = df_valid['pixels'].apply(lambda x: np.fromstring(x, sep=' ', dtype=np.float32))\n",
    "    valid_targets = df_valid[f'class_{fold}'].values\n",
    "\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    train_aug = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=224, width=224),  # Resize images to 224x224\n",
    "        A.Normalize(mean=mean, std=std, max_pixel_value=255.0, always_apply=True),\n",
    "        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15),\n",
    "        A.HorizontalFlip(p=0.5)\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    valid_aug = A.Compose(\n",
    "        [\n",
    "        A.Resize(height=224, width=224),  # Resize images to 224x224\n",
    "        A.Normalize(mean=mean, std=std, max_pixel_value=255.0, always_apply=True)\n",
    "        ]\n",
    "    )\n",
    "    train_dataset = CustomImageDataset(\n",
    "        pixel_arrays=train_pixels,\n",
    "        targets=train_targets,\n",
    "        resize=(224, 224),  # New size\n",
    "        augmentations=train_aug,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = CustomImageDataset(\n",
    "        pixel_arrays=valid_pixels,\n",
    "        targets=valid_targets,\n",
    "        resize=(224, 224),  # New size\n",
    "        augmentations=valid_aug,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=32, shuffle=False, num_workers=4\n",
    "    )\n",
    "    model = load_model(2)\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize CrossEntropyLoss and CenterLoss\n",
    "    nllloss = nn.CrossEntropyLoss().cuda()\n",
    "    centerloss = CenterLoss(num_classes=2, feat_dim=2048, use_gpu=True).cuda()\n",
    "    loss_weight = 0.05 # Weight for the center loss\n",
    "\n",
    "    # Initialize optimizers\n",
    "    optimizer4nn = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
    "    optimizer4center = torch.optim.Adagrad(centerloss.parameters(), lr=0.001)\n",
    "    optimizer = [optimizer4nn, optimizer4center]\n",
    "\n",
    "    # Create separate schedulers for both optimizers\n",
    "    scheduler_nn = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer4nn, patience=3, threshold=0.001, mode=\"max\"\n",
    "    )\n",
    "    \n",
    "    scheduler_center = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer4center, patience=3, threshold=0.001, mode=\"max\"\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=5, mode=\"max\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train the model using the updated function\n",
    "        train_loss = Engine.train(\n",
    "            train_loader, model, optimizer, device, centerloss, nllloss, loss_weight\n",
    "        )\n",
    "\n",
    "        # Validation step\n",
    "        predictions, valid_loss = Engine.evaluate(valid_loader, model, device=device)\n",
    "        final_predictions = np.argmax(np.vstack(predictions), axis=1)\n",
    "        valid_targets = np.array(valid_targets)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(valid_targets, final_predictions)\n",
    "        print(f\"Epoch = {epoch}, Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        # Step both schedulers based on the validation accuracy\n",
    "        scheduler_nn.step(accuracy)\n",
    "        scheduler_center.step(accuracy)\n",
    "\n",
    "        # Early stopping\n",
    "        es(accuracy, model, model_path=f\"model_fold_RE152class{fold}.bin\")\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Return OOF data\n",
    "    oof_data = {\n",
    "        'id': df_valid.index,\n",
    "        'true_emotion': valid_targets,\n",
    "        'pred_emotion': final_predictions\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(oof_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aaad462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T05:54:22.325067Z",
     "iopub.status.busy": "2024-09-14T05:54:22.324758Z",
     "iopub.status.idle": "2024-09-14T08:54:58.255503Z",
     "shell.execute_reply": "2024-09-14T08:54:58.254371Z"
    },
    "papermill": {
     "duration": 10835.939531,
     "end_time": "2024-09-14T08:54:58.257865",
     "exception": false,
     "start_time": "2024-09-14T05:54:22.318334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
      "100%|██████████| 230M/230M [00:00<00:00, 294MB/s]\n",
      "  0%|          | 0/215 [00:00<?, ?it/s]/tmp/ipykernel_24/3583249165.py:22: UserWarning: This overload of addmm_ is deprecated:\n",
      "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta = 1, Number alpha = 1) (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/python_arg_parser.cpp:1581.)\n",
      "  distmat.addmm_(1, -2, x, self.centers.t())\n",
      "100%|██████████| 215/215 [02:35<00:00,  1.39it/s, loss=72.6]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.16it/s, loss=0.478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.7840\n",
      "Validation score improved (-inf --> 0.784). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:42<00:00,  1.33it/s, loss=60]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.8310\n",
      "Validation score improved (0.784 --> 0.831). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:42<00:00,  1.32it/s, loss=56]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.19it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.8590\n",
      "Validation score improved (0.831 --> 0.859). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:42<00:00,  1.32it/s, loss=53.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.21it/s, loss=0.375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.8690\n",
      "Validation score improved (0.859 --> 0.869). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:42<00:00,  1.32it/s, loss=51.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.8750\n",
      "Validation score improved (0.869 --> 0.875). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:42<00:00,  1.32it/s, loss=50]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.415]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.8670\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:43<00:00,  1.32it/s, loss=48.9]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.8490\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:42<00:00,  1.32it/s, loss=47.8]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.14it/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, Accuracy = 0.8620\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:43<00:00,  1.32it/s, loss=47.2]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, Accuracy = 0.8620\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [02:43<00:00,  1.32it/s, loss=46.6]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9, Accuracy = 0.8640\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=65.1]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.9790\n",
      "Validation score improved (-inf --> 0.979). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=54.3]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.15it/s, loss=0.145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.9500\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=52]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.15it/s, loss=0.0764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.9850\n",
      "Validation score improved (0.979 --> 0.985). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:06<00:00,  1.32it/s, loss=50.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.0771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9860\n",
      "Validation score improved (0.985 --> 0.986). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=49.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.07it/s, loss=0.0802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.9870\n",
      "Validation score improved (0.986 --> 0.987). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=48.9]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.0839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9860\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=48.3]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.13it/s, loss=0.0916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.9850\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=47.6]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.16it/s, loss=0.0941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, Accuracy = 0.9850\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=47]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.19it/s, loss=0.0872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, Accuracy = 0.9880\n",
      "Validation score improved (0.987 --> 0.988). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:06<00:00,  1.32it/s, loss=46.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.0934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9, Accuracy = 0.9880\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:06<00:00,  1.32it/s, loss=46.1]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.0973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10, Accuracy = 0.9850\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:06<00:00,  1.32it/s, loss=45.7]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.09]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11, Accuracy = 0.9870\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:06<00:00,  1.32it/s, loss=45.2]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.19it/s, loss=0.0908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12, Accuracy = 0.9880\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [03:07<00:00,  1.32it/s, loss=45.1]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.0918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13, Accuracy = 0.9870\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.31it/s, loss=70.1]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.21it/s, loss=0.481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.7920\n",
      "Validation score improved (-inf --> 0.792). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=58.8]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.522]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.7660\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=54.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.8190\n",
      "Validation score improved (0.792 --> 0.819). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=51.4]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.7750\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=49.4]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.8380\n",
      "Validation score improved (0.819 --> 0.838). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=47.9]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.8310\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=46.8]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.16it/s, loss=0.526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.8520\n",
      "Validation score improved (0.838 --> 0.852). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=45.7]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.15it/s, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, Accuracy = 0.8510\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.31it/s, loss=45]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.14it/s, loss=0.583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, Accuracy = 0.8390\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.31it/s, loss=44.2]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.16it/s, loss=0.598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9, Accuracy = 0.8520\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.31it/s, loss=43.7]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10, Accuracy = 0.8530\n",
      "Validation score improved (0.852 --> 0.853). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=43.2]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.14it/s, loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11, Accuracy = 0.8430\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.31it/s, loss=42.6]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12, Accuracy = 0.8380\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=42.2]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13, Accuracy = 0.8470\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.31it/s, loss=41.9]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.15it/s, loss=0.655]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 14, Accuracy = 0.8510\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:42<00:00,  1.32it/s, loss=41.4]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 15, Accuracy = 0.8460\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:22<00:00,  1.32it/s, loss=74]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.8140\n",
      "Validation score improved (-inf --> 0.814). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=62.7]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.15it/s, loss=0.305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.8790\n",
      "Validation score improved (0.814 --> 0.879). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=58.9]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.16it/s, loss=0.321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.8760\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=57.1]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.9110\n",
      "Validation score improved (0.879 --> 0.911). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=55.7]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.8300\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=54.9]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.9020\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=53.3]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.14it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.9050\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=52.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, Accuracy = 0.9130\n",
      "Validation score improved (0.911 --> 0.913). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=51.7]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, Accuracy = 0.9190\n",
      "Validation score improved (0.913 --> 0.919). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=51.1]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 9, Accuracy = 0.9110\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=50.4]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.16it/s, loss=0.367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 10, Accuracy = 0.9050\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=49.8]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 11, Accuracy = 0.9080\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=49.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 12, Accuracy = 0.8990\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 187/187 [02:21<00:00,  1.32it/s, loss=49.1]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 13, Accuracy = 0.9140\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:39<00:00,  1.32it/s, loss=73.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.20it/s, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0, Accuracy = 0.7810\n",
      "Validation score improved (-inf --> 0.781). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:38<00:00,  1.32it/s, loss=63.5]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.16it/s, loss=0.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1, Accuracy = 0.7070\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:38<00:00,  1.32it/s, loss=59]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.22it/s, loss=1.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 2, Accuracy = 0.2800\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:38<00:00,  1.32it/s, loss=55.8]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.15it/s, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 3, Accuracy = 0.8240\n",
      "Validation score improved (0.781 --> 0.824). Saving model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:38<00:00,  1.32it/s, loss=53.8]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.15it/s, loss=0.554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 4, Accuracy = 0.7860\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:39<00:00,  1.32it/s, loss=52]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.703]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 5, Accuracy = 0.7500\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:38<00:00,  1.32it/s, loss=50.6]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.17it/s, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 6, Accuracy = 0.7790\n",
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:39<00:00,  1.32it/s, loss=49.7]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.08it/s, loss=0.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 7, Accuracy = 0.8240\n",
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [02:39<00:00,  1.32it/s, loss=49.1]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.18it/s, loss=0.608]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 8, Accuracy = 0.8100\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.empty((0, 3))  \n",
    "\n",
    "for i in range(0, 5):\n",
    "    df = train(i)\n",
    "    df.to_csv(f\"oof1EFOCAL-{i}.csv\", index=False, header=False)  \n",
    "    X = np.concatenate((X, df.values))  \n",
    "df = pd.DataFrame(X, columns=['id', 'true_emotion', 'pred_emotion']) \n",
    "df.to_csv('RES152B.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bf6c7",
   "metadata": {
    "papermill": {
     "duration": 2.527836,
     "end_time": "2024-09-14T08:55:03.366288",
     "exception": false,
     "start_time": "2024-09-14T08:55:00.838452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7504fef",
   "metadata": {
    "papermill": {
     "duration": 2.499893,
     "end_time": "2024-09-14T08:55:08.364182",
     "exception": false,
     "start_time": "2024-09-14T08:55:05.864289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5686538,
     "sourceId": 9375078,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10877.616533,
   "end_time": "2024-09-14T08:55:13.751589",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-14T05:53:56.135056",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
